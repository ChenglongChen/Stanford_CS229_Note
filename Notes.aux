\relax 
\citation{MatrixDiffCalc}
\@writefile{toc}{\contentsline {section}{\numberline {1}Notes on: Lecture notes 1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Page 8: $\text  {tr}AB=\text  {tr}BA$}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Page 9: $\nabla _{A}\text  {tr}AB=B^T$}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Page 9: $\nabla _{A^T}f(A)=(\nabla _{A}f(A))^T$}{1}}
\citation{MatrixDiffCalc}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Page 9: $\nabla _{A}\text  {tr}ABA^TC=CAB+C^TAB^T$}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Page 11: Another method to derive $\nabla _{\theta }J(\theta )$}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Page 15: Locally weighted linear regression and Non-parametric methods}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Regularized Linear Regression (From Ng's ML course of Coursera)}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}The invertibility of $A^TA$}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}The invertibility of $A^TA+\lambda I$}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notes on: Lecture notes 2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Page 1: Why does logistic regression try to find a straight line?}{5}}
\citation{Conditional}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Page 3: The two definitions of covariance}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Page 6: Derivation of the MLE of the parameters of GDA}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Page 6: Derivation of the decision boundary of GDA}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Page 9: Independent and conditionally independent}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Page 10: Derivation of the MLE of the parameters of Naive Bayes}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Page 14: Derivation of the MLE of the parameters of multinomial event model}{7}}
\citation{PRML}
\@writefile{toc}{\contentsline {section}{\numberline {3}Notes on: Lecture notes 3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Page 4: $w$ is orthogonal (at $90^\circ $) to the separating hyperplane}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Page 13: The optimal value for the intercept term $b$}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Page 20: The dual form of the $\ell _1$ norm soft margin SVM}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Page 20: The KKT dual complementary conditions of the $\ell _1$ norm soft margin SVM}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Notes on: Lecture notes 4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Page 6: The size of training set $m$ (sample complexity)}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Page 7: The error bound $\gamma $}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Page 8: Corollary}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Notes on: Lecture notes 5}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Page 7: The posterior distribution $p(y|x,S)$}{12}}
\@writefile{toc}{\contentsline {paragraph}{The distribution $p(\theta |x,S)$}{12}}
\@writefile{toc}{\contentsline {paragraph}{The distribution $p(y|\theta ,x,S)$}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Notes on: Lecture notes 7a}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Implementation of $k$-means}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Class Assignment}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Centroid Update}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Distortion function}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}The overall implementation of $k$-means}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Notes on: Lecture notes 7b}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Page 2: The ML estimates of Gaussian Mixture Model}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Notes on: Lecture notes 8}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Page 1: 1-D proof of Jensen's inequality}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Page 6-8: The EM algorithm of Gaussian Mixture Model}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Derivation of the EM algorithm of Naive Bayes}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Notes on: Lecture notes 9}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Page 1: The covariance matrix $\Sigma $ is singular}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Page 2: The MLE for restriction of $\Sigma $ being diagonal}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Page 2: The MLE for restriction of $\Sigma $ being diagonal with equal entries}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Page 7: The derivation of EM for factor analysis}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}The M-step update for $\Lambda $}{28}}
\@writefile{toc}{\contentsline {paragraph}{First part: $\frac  {\partial }{\partial \Lambda }(z^{(i)})^T\Lambda ^T\Psi ^{-1}\Lambda z^{(i)}$}{29}}
\@writefile{toc}{\contentsline {paragraph}{Second part: $\frac  {\partial }{\partial \Lambda }(z^{(i)})^T\Lambda ^T\Psi ^{-1}(x^{(i)}-\mu )$}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.2}The M-step update for $\mu $}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.3}The M-step update for $\Psi $}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Notes on: Lecture notes 10}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Page 5: The derivation of PCA}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Page 5: The orthogonal property of eigenvectors of $\Sigma $}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Computation of eigenvectors of $\Sigma $}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Use \texttt  {eig} routine}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Use \texttt  {svd} routine}{33}}
\@writefile{toc}{\contentsline {paragraph}{Unitary matrix}{33}}
\@writefile{toc}{\contentsline {paragraph}{Singular value}{33}}
\@writefile{toc}{\contentsline {paragraph}{$A^TA$ and $AA^T$ have the same non-zero eigenvalues}{33}}
\@writefile{toc}{\contentsline {paragraph}{Singular Value Decomposition}{34}}
\@writefile{toc}{\contentsline {paragraph}{Method 1 using \texttt  {svd}}{34}}
\@writefile{toc}{\contentsline {paragraph}{Method 2 using \texttt  {svd}}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Page 5: Projecting and recovering the data}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Page 6: The first $k$ principal components maximize $\DOTSB \sum@ \slimits@ _i ||y^{(i)}||_2^2$}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Collections}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Miscellaneous}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Confusion Matrix}{36}}
\bibstyle{plain}
\bibdata{NotesReference}
\bibcite{PRML}{1}
\bibcite{Conditional}{2}
\bibcite{MatrixDiffCalc}{3}
